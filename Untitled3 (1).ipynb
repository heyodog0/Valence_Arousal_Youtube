{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2672a708-6dec-42d9-a71b-553f93881b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/heyodogo/Documents/DrOng/jupyter/class-env/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96f80378f51487c8e11f9c871a2ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 1]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Reshape targets if necessary (should already be same size as predictions)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mlogits, targets)\n\u001b[1;32m     90\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 1]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW, get_scheduler\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset Paths (replace with your actual file paths)\n",
    "data_files = {\"train\": \"/Users/heyodogo/Downloads/emobank_train.csv\", \"test\": \"/Users/heyodogo/Downloads/emobank_test.csv\"}\n",
    "\n",
    "# Load the Dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Remove unnecessary columns and rename the label column\n",
    "dataset = dataset.remove_columns(['id', 'A', 'D', 'split'])\n",
    "dataset = dataset.rename_column(\"V\", \"labels\")\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Load tokenizer (replace 'bert-base-uncased' with appropriate model if needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the Dataset (batched)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove \"text\" column and set format to torch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Sample Set Size (consider increasing for better training)\n",
    "SAMPLE_SET_SIZE = 300  # Increase this if possible\n",
    "\n",
    "# Create Train and Eval Datasets (shuffled and selected)\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Load Pre-trained Model (consider DistilBERT for smaller datasets)\n",
    "model_name = \"bert-base-uncased\"  # Or a pre-trained regression model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Move Model to Device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # Experiment with learning rates\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Loss Function (Ensure labels are continuous valence values)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # Ensure labels are floats (continuous values)\n",
    "    targets = batch['labels'].float().to(device)\n",
    "    # Reshape targets if necessary (should already be same size as predictions)\n",
    "    targets = targets.view(len(batch), 1)\n",
    "\n",
    "    loss = loss_fn(outputs.logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Evaluation\n",
    "metric = evaluate.load(\"mse\")  # Mean Squared Error for regression\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  with torch.no_no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e456fb6-fce0-4b73-8b6b-b1a5584a9db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "targets = targets.view(len(batch), 1)  # Reshape to match batch size\n",
    "# Inside the training loop, before reshaping\n",
    "print(targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74aa899a-d81e-4d64-8c15-4e8408efd12c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/heyodogo/Documents/DrOng/jupyter/class-env/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7919d04ca4cb4a21d5117114d34c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heyodogo/Documents/DrOng/jupyter/class-env/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 7.27}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW, get_scheduler\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset Paths (replace with your actual file paths)\n",
    "data_files = {\"train\": \"/Users/heyodogo/Downloads/emobank_train.csv\", \"test\": \"/Users/heyodogo/Downloads/emobank_test.csv\"}\n",
    "\n",
    "# Load the Dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Remove unnecessary columns and rename the label column\n",
    "dataset = dataset.remove_columns(['id', 'A', 'D', 'split'])\n",
    "dataset = dataset.rename_column(\"V\", \"labels\")\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Load tokenizer (replace 'bert-base-uncased' with appropriate model if needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the Dataset (batched)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove \"text\" column and set format to torch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# # Sample Set Size (consider increasing for better training)\n",
    "# SAMPLE_SET_SIZE = 100  # Increase this if possible\n",
    "\n",
    "# # Create Train and Eval Datasets (shuffled and selected)\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Load Pre-trained Model (consider DistilBERT for smaller datasets)\n",
    "model_name = \"bert-base-uncased\"  # Or a pre-trained regression model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Move Model to Device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # Experiment with learning rates\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Loss Function (Ensure labels are continuous valence values)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # Ensure labels are floats (continuous values)\n",
    "    targets = batch['labels'].float().to(device)\n",
    "\n",
    "    # **Reshape targets with unsqueeze to add a dimension (if necessary)**\n",
    "    targets = targets.unsqueeze(1)  # Add a dimension of size 1\n",
    "\n",
    "    loss = loss_fn(outputs.logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Evaluation\n",
    "metric = evaluate.load(\"mse\")  # Mean Squared Error for regression\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c5f468-b9fe-4036-93ff-4369ed0836c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heyodogo/Documents/DrOng/jupyter/class-env/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 7.27}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"mse\")  # Mean Squared Error for regression\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4f106-9be6-4770-8d44-bdd6c01b4ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class-kernel",
   "language": "python",
   "name": "class-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
